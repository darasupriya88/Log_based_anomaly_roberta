{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07fbf932",
   "metadata": {},
   "source": [
    "### https://github.com/logpai/loglizer?tab=readme-ov-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e14a1242",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import OrderedDict\n",
    "from collections import Counter\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "caa4e400",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_event_ids(data_frame, regex_pattern, column_names):\n",
    "    \"\"\"\n",
    "    turns input data_frame into a 2 columned dataframe\n",
    "    with columns: BlockId, EventSequence\n",
    "    where EventSequence is a list of the events that happened to the block\n",
    "    \"\"\"\n",
    "    data_dict = OrderedDict()\n",
    "    for _, row in data_frame.iterrows():\n",
    "        blk_id_list = re.findall(regex_pattern, row[\"Content\"])\n",
    "        blk_id_set = set(blk_id_list)\n",
    "        for blk_id in blk_id_set:\n",
    "            if blk_id not in data_dict:\n",
    "                data_dict[blk_id] = []\n",
    "            data_dict[blk_id].append(row[\"EventId\"])\n",
    "    data_df = pd.DataFrame(list(data_dict.items()), columns=column_names)\n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98b35e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def windower(sequence, window_size):\n",
    "    \"\"\"\n",
    "    creates an array of arrays of windows\n",
    "    output array is of length: len(sequence) - window_size + 1\n",
    "    \"\"\"\n",
    "    return np.lib.stride_tricks.sliding_window_view(sequence, window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5485dfe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sequence_padder(sequence, required_length):\n",
    "    \"\"\"\n",
    "    right pads events sequence until max sequence length long\n",
    "    \"\"\"\n",
    "    if len(sequence) > required_length:\n",
    "        return sequence\n",
    "    return np.pad(\n",
    "        sequence,\n",
    "        (0, required_length - len(sequence)),\n",
    "        mode=\"constant\",\n",
    "        constant_values=(0),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31e9fb23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def resize_time_image(time_image, size):\n",
    "    \"\"\"\n",
    "    compresses time images that had more sequences than the set max sequence length\n",
    "    \"\"\"\n",
    "    width = size[1]\n",
    "    height = size[0]\n",
    "    return np.array(Image.fromarray(time_image).resize((width, height)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9990b34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureExtractor(object):\n",
    "    \"\"\"\n",
    "    class for fitting and transforming the training set\n",
    "    then transforming the testing set\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.mean_vec = None\n",
    "        self.idf_vec = None\n",
    "        self.events = None\n",
    "        self.term_weighting = None\n",
    "        self.max_seq_length = None\n",
    "        self.window_size = None\n",
    "        self.num_rows = None\n",
    "\n",
    "    def fit_transform(\n",
    "        self, X_seq, term_weighting=None, length_percentile=90, window_size=16\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Fit and transform the training set\n",
    "        X_Seq: ndarray,  log sequences matrix\n",
    "        term_weighting: None or `tf-idf`\n",
    "        length_percentile: int, set the max length of the event sequences\n",
    "        window_size: int, size of subsetting\n",
    "        \"\"\"\n",
    "        self.term_weighting = term_weighting\n",
    "        self.window_size = window_size\n",
    "\n",
    "        # get unique events\n",
    "        self.events = list(set(np.concatenate(X_seq).ravel().flatten()))\n",
    "\n",
    "        # get lengths of event sequences\n",
    "        length_list = np.array(list(map(len, X_seq)))\n",
    "        self.max_seq_length = int(np.percentile(length_list, length_percentile))\n",
    "\n",
    "        self.num_rows = self.max_seq_length - self.window_size + 1\n",
    "\n",
    "        print(\"final shape will be \", self.num_rows, len(self.events))\n",
    "\n",
    "        # loop over each sequence to create the time image\n",
    "        time_images = []\n",
    "        for block in X_seq:\n",
    "            padded_block = sequence_padder(block, self.max_seq_length)\n",
    "            time_image = windower(padded_block, self.window_size)\n",
    "            time_image_counts = []\n",
    "            for time_row in time_image:\n",
    "                row_count = Counter(time_row)\n",
    "                time_image_counts.append(row_count)\n",
    "\n",
    "            time_image_df = pd.DataFrame(time_image_counts, columns=self.events)\n",
    "            time_image_df = time_image_df.reindex(sorted(time_image_df.columns), axis=1)\n",
    "            time_image_df = time_image_df.fillna(0)\n",
    "            time_image_np = time_image_df.to_numpy()\n",
    "\n",
    "            # resize if too large\n",
    "            if len(time_image_np) > self.num_rows:\n",
    "                time_image_np = resize_time_image(\n",
    "                    time_image_np, (self.num_rows, len(self.events)),\n",
    "                )\n",
    "\n",
    "            time_images.append(time_image_np)\n",
    "\n",
    "        # stack all the blocks\n",
    "        X = np.stack(time_images)\n",
    "\n",
    "        if self.term_weighting == \"tf-idf\":\n",
    "\n",
    "            # set up sizing\n",
    "            dim1, dim2, dim3 = X.shape\n",
    "            X = X.reshape(-1, dim3)\n",
    "\n",
    "            # apply tf-idf\n",
    "            df_vec = np.sum(X > 0, axis=0)\n",
    "            self.idf_vec = np.log(dim1 / (df_vec + 1e-8))\n",
    "            idf_tile = np.tile(self.idf_vec, (dim1 * dim2, 1))\n",
    "            idf_matrix = X * idf_tile\n",
    "            X = idf_matrix\n",
    "\n",
    "            # reshape to original dimensions\n",
    "            X = X.reshape(dim1, dim2, dim3)\n",
    "\n",
    "        X_new = X\n",
    "        print(\"train data shape: \", X_new.shape)\n",
    "        return X_new\n",
    "\n",
    "    def transform(self, X_seq):\n",
    "        \"\"\"\n",
    "        transforms x test\n",
    "        X_seq : log sequence data\n",
    "        \"\"\"\n",
    "\n",
    "        # loop over each sequence to create the time image\n",
    "        time_images = []\n",
    "        for block in X_seq:\n",
    "            padded_block = sequence_padder(block, self.max_seq_length)\n",
    "            time_image = windower(padded_block, self.window_size)\n",
    "            time_image_counts = []\n",
    "            for time_row in time_image:\n",
    "                row_count = Counter(time_row)\n",
    "                time_image_counts.append(row_count)\n",
    "\n",
    "            time_image_df = pd.DataFrame(time_image_counts, columns=self.events)\n",
    "            time_image_df = time_image_df.reindex(sorted(time_image_df.columns), axis=1)\n",
    "            time_image_df = time_image_df.fillna(0)\n",
    "            time_image_np = time_image_df.to_numpy()\n",
    "\n",
    "            # resize if too large\n",
    "            if len(time_image_np) > self.num_rows:\n",
    "                time_image_np = resize_time_image(\n",
    "                    time_image_np, (self.num_rows, len(self.events)),\n",
    "                )\n",
    "\n",
    "            time_images.append(time_image_np)\n",
    "\n",
    "        # stack all the blocks\n",
    "        X = np.stack(time_images)\n",
    "\n",
    "        if self.term_weighting == \"tf-idf\":\n",
    "\n",
    "            # set up sizing\n",
    "            dim1, dim2, dim3 = X.shape\n",
    "            X = X.reshape(-1, dim3)\n",
    "\n",
    "            # apply tf-idf\n",
    "            idf_tile = np.tile(self.idf_vec, (dim1 * dim2, 1))\n",
    "            idf_matrix = X * idf_tile\n",
    "            X = idf_matrix\n",
    "\n",
    "            # reshape to original dimensions\n",
    "            X = X.reshape(dim1, dim2, dim3)\n",
    "\n",
    "        X_new = X\n",
    "        print(\"test data shape: \", X_new.shape)\n",
    "        return X_new\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9b8da59",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading x_train\n",
      "loading x_test\n",
      "loading y\n",
      "collecting events for x_train\n",
      "collecting events for x_test\n",
      "merging block frames with labels\n",
      "removing blocks that are overlapped into train and test\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    data_version = \"_v5\"\n",
    "\n",
    "    data_version = \"_tf-idf{}\".format(data_version)\n",
    "\n",
    "    # where the \"raw\" data for this file is located\n",
    "    load_data_location = \"./project_processed_data/\"\n",
    "\n",
    "    # where the processed data is saved\n",
    "    save_location = \"./project_processed_data/{}/\".format(data_version)\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Loads data\n",
    "    print(\"loading x_train\")\n",
    "    x_train = pd.read_csv(\"{}HDFS_train.log_structured.csv\".format(load_data_location))\n",
    "\n",
    "    print(\"loading x_test\")\n",
    "    x_test = pd.read_csv(\"{}HDFS_test.log_structured.csv\".format(load_data_location))\n",
    "\n",
    "    print(\"loading y\")\n",
    "    y = pd.read_csv(\"{}anomaly_label.csv\".format(load_data_location))\n",
    "\n",
    "    # processes events into blocks\n",
    "    re_pat = r\"blk_-[0-9]*\"\n",
    "    col_names = [\"BlockId\", \"EventSequence\"]\n",
    "\n",
    "    print(\"collecting events for x_train\")\n",
    "    events_train = collect_event_ids(x_train, re_pat, col_names)\n",
    "    print(\"collecting events for x_test\")\n",
    "    events_test = collect_event_ids(x_test, re_pat, col_names)\n",
    "\n",
    "    print(\"merging block frames with labels\")\n",
    "    events_train = events_train.merge(y, on=\"BlockId\")\n",
    "    events_test = events_test.merge(y, on=\"BlockId\")\n",
    "\n",
    "    print(\"removing blocks that are overlapped into train and test\")\n",
    "    overlapping_blocks = np.intersect1d(events_train[\"BlockId\"], events_test[\"BlockId\"])\n",
    "    events_train = events_train[~events_train[\"BlockId\"].isin(overlapping_blocks)]\n",
    "    events_test = events_test[~events_test[\"BlockId\"].isin(overlapping_blocks)]\n",
    "\n",
    "    events_train_values = events_train[\"EventSequence\"].values\n",
    "    events_test_values = events_test[\"EventSequence\"].values\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "0a74ed48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fit_transform x_train\n",
      "final shape will be  16 43\n",
      "train data shape:  (197301, 16, 43)\n",
      "transform x_test\n",
      "test data shape:  (60336, 16, 43)\n",
      "collecting y data\n"
     ]
    }
   ],
   "source": [
    "# fit transform & transform\n",
    "fe = FeatureExtractor()\n",
    "\n",
    "print(\"fit_transform x_train\")\n",
    "subblocks_train = fe.fit_transform(\n",
    "    events_train_values,\n",
    "    term_weighting=\"tf-idf\",\n",
    "    length_percentile=95,\n",
    "    window_size=16,\n",
    ")\n",
    "\n",
    "print(\"transform x_test\")\n",
    "subblocks_test = fe.transform(events_test_values)\n",
    "\n",
    "print(\"collecting y data\")\n",
    "y_train = events_train[[\"BlockId\", \"Label\"]]\n",
    "y_test = events_test[[\"BlockId\", \"Label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c530ba1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "writing y to csv\n",
      "saving x to numpy object\n",
      "time taken : 87644.292844961121\n"
     ]
    }
   ],
   "source": [
    "# saving files\n",
    "print(\"writing y to csv\")\n",
    "y_train.to_csv(\"{}y_train{}.csv\".format(save_location, data_version))\n",
    "y_test.to_csv(\"{}y_test{}.csv\".format(save_location, data_version))\n",
    "\n",
    "print(\"saving x to numpy object\")\n",
    "np.save(\"{}x_train{}.npy\".format(save_location, data_version), subblocks_train)\n",
    "np.save(\"{}x_test{}.npy\".format(save_location, data_version), subblocks_test)\n",
    "\n",
    "print(\"time taken :\", time.time() - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
